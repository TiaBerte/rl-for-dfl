{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1f4a83",
   "metadata": {},
   "source": [
    "# Decision-focused Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55980a77",
   "metadata": {},
   "source": [
    "## Prediction and Optimization in the Wild\n",
    "\n",
    "**Real world problems typically rely on _estimated parameters_**\n",
    "\n",
    "E.g. travel times, demands, item weights/costs...\n",
    "\n",
    "<center><img src=\"assets/traffic.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "**However, sometimes we have access to _a bit more information_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde22799",
   "metadata": {},
   "source": [
    "## Prediction and Optimization in the Wild\n",
    "\n",
    "**Take _traffic-dependent travel times_ as an example**\n",
    "\n",
    "If we know the _time of the day_ we can probably estimate them better\n",
    "\n",
    "<center><img src=\"assets/traffic.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "**Let's see how these problems are typically addressed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d631a",
   "metadata": {},
   "source": [
    "## Predict, then Optimize\n",
    "\n",
    "**First, we _train an estimator_ for the problem parameters:**\n",
    "\n",
    "$$\n",
    "\\text{argmin}_{\\omega} \\left\\{L(y, \\hat{y}) \\mid y = f(\\hat{x}; \\omega) \\right\\}\n",
    "$$\n",
    "* $L$ is the loss function\n",
    "* $f$ is the ML model with parameter \n",
    "* $\\hat{x}$, $\\hat{y}$ are the training set input/output\n",
    "\n",
    "**In our example:**\n",
    "\n",
    "* $x$ would be the time of the day\n",
    "* $y$ would be a vector of travel times\n",
    "* $L$ may be a classical MSE loss\n",
    "* $f$ may be a linear regressor or neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6143b",
   "metadata": {},
   "source": [
    "## Predict, then Optimize\n",
    "\n",
    "**Then, we solve the optimization problem with the estimated parameters**\n",
    "\n",
    "$$\n",
    "z^*(y) = \\text{argmin}_z \\left\\{ c(z, y) \\mid z \\in F(y) \\right\\}\n",
    "$$\n",
    "* $z$ is the vector of variables of the optimization problem\n",
    "* $c$ is the cost function\n",
    "* $F$ is the feasible space\n",
    "* In general, both $c$ and $F$ may depend on the estimated parameters\n",
    "\n",
    "**In our example**\n",
    "\n",
    "* $z$ may represent routing decisions\n",
    "* $c$ may be the total travel time\n",
    "* $F$ may encode a deadline constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d494ef",
   "metadata": {},
   "source": [
    "## Predict, then Optimize\n",
    "\n",
    "**This approach is sometimes referred to as \"Predict, then Optimize\"**\n",
    "\n",
    "It is simple and it makes intuitively sense\n",
    "\n",
    "* The more accurate we are, the better we will estimate the parameters\n",
    "* ...And in turn we should get better optimization results\n",
    "\n",
    "**Let $L^*(\\hat{y})$ be the best possible loss value, for any ML model**\n",
    "\n",
    "* For any reasonable loss function, _better training leads to better predictions_\n",
    "\n",
    "$$\n",
    "y \\xrightarrow[L(y, \\hat{y}) \\rightarrow L^*(\\hat{y})]{} y^*\n",
    "$$\n",
    "\n",
    "* ...And therefore, eventually we are _guaranteed to find the best solution_\n",
    "\n",
    "$$\n",
    "z^*(y) \\xrightarrow[L(y, \\hat{y}) \\rightarrow L^*(\\hat{y})]{} z^*(\\hat{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf941ce",
   "metadata": {},
   "source": [
    "## \"Predict, then Optimize\": Limitations\n",
    "\n",
    "**However, things are not really that simple!**\n",
    "\n",
    "> **Why is that the case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dd5ff",
   "metadata": {},
   "source": [
    "The relation:\n",
    "\n",
    "$$\n",
    "z^*(y) \\xrightarrow[L(y, \\hat{y}) \\rightarrow L^*(\\hat{y})]{} z^*(\\hat{y})\n",
    "$$\n",
    "\n",
    "...Holds only _asymptotically_\n",
    "\n",
    "* In practice, our model may not be capable of reaching mimimum loss\n",
    "* ...And this is even more true for unseen example\n",
    "\n",
    "In this situation, it is unclear _how imperfect predictions impact the cost_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585354",
   "metadata": {},
   "source": [
    "## \"Predict, then Optimize\": Limitations\n",
    "\n",
    "**Say we want to move from location A to B, using one of two routes**\n",
    "\n",
    "Based on the time of the day (x-axis)\n",
    "\n",
    "<center class='smalltext'>\n",
    "<img src=\"assets/spo_example_1.png\" width=\"45%\"/>\n",
    "Image from <a href=\"https://arxiv.org/abs/1710.08005\">\"Smart Predict, then Optimize\"</a>\n",
    "</center>\n",
    "\n",
    "...The travel time changes (y-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465f735",
   "metadata": {},
   "source": [
    "## \"Predict, then Optimize\": Limitations\n",
    "\n",
    "**We need to pick the best route**\n",
    "\n",
    "<center class='smalltext'>\n",
    "<img src=\"assets/spo_example_1.png\" width=\"45%\"/>\n",
    "Image from <a href=\"https://arxiv.org/abs/1710.08005\">\"Smart Predict, then Optimize\"</a>\n",
    "</center>\n",
    "\n",
    "* The dashed line shows the input value that causes the optimal choice to switch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d27d8",
   "metadata": {},
   "source": [
    "## \"Predict, then Optimize\": Limitations\n",
    "\n",
    "**If we train an optimal Linear Regression, we get these estimates**\n",
    "\n",
    "<center><img src=\"assets/spo_example_2.png\" width=\"40%\"/></center>\n",
    "\n",
    "* The estimator is most accurate possible\n",
    "* ...But we get the switching point _wrong_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d15fa",
   "metadata": {},
   "source": [
    "## \"Predict, then Optimize\": Limitations\n",
    "\n",
    "**By contrast, consider this second estimator**\n",
    "\n",
    "<center><img src=\"assets/spo_example_3.png\" width=\"40%\"/></center>\n",
    "\n",
    "* The accuracy is awful\n",
    "* ...But we get the switching point _right_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea85709",
   "metadata": {},
   "source": [
    "## Decision Focused Learning\n",
    "\n",
    "**Addressing these issues is the goal of _decision focused learning_**\n",
    "\n",
    "* The general idea is to _account for the optimization problem_ during training\n",
    "* ...And the \"holy grail\" of the DFL is solving:\n",
    "\n",
    "$$\n",
    "\\text{argmin}_{\\omega} \\left\\{ \\sum_{i=1}^m c(z^*(y_i), \\hat{y}_i) \\mid y = f(\\hat{x}, \\omega) \\right\\}\n",
    "$$\n",
    "\n",
    "* The field was kicked off by [this paper](https://arxiv.org/abs/1703.04529)\n",
    "* ...And many other have followed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b473ca",
   "metadata": {},
   "source": [
    "## The Main Challenge\n",
    "\n",
    "**We are now ready to tackled decision-focused learning**\n",
    "\n",
    "Let's start from the \"holy grail\" problem:\n",
    "\n",
    "$$\n",
    "\\text{argmin}_{\\omega} \\left\\{ \\sum_{i=1}^m c(z^*(y_i), \\hat{y}_i) \\mid y = f(\\hat{x}, \\omega) \\right\\}\n",
    "$$\n",
    "\n",
    "Unfortunately, the $\\text{argmin}$ used to define $z^*(y_i)$ is _non-differentiable_\n",
    "\n",
    "* A small change in the prediction vector $y_i$\n",
    "* ...May cause a large/discrete change in the optimal solution $z_i$\n",
    "\n",
    "This is certainly the case for our example problem\n",
    "\n",
    "> **How are we going to deal with this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abaed2d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for DFL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd467c",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) researchers have been working for years on developing methods theat deal with non-differentiable rewards.\n",
    "\n",
    "**What if the action of the agent is the predicted parameter $\\hat{y}_i$ and the reward is the cost of the solution $c(z^*(y), \\hat{y})$?**\n",
    "\n",
    "In this way, we are solving the DFL problem and in principle we can use any RL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b053178",
   "metadata": {},
   "source": [
    "## Weighted Set Multi-cover with stochastic coverage requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6a329",
   "metadata": {},
   "source": [
    "Now we introduce the problem we will investigate. We consider a simplified production planning problem, modeled as a Set Multi-cover Problem with stochastic coverage requirements. Given a universe $N$ containing $n$ elements and a collection of sets over $N$, the Set Multi-cover Problem requires finding a minimum size sub-collection of the sets such that coverage requirements for each element are satisfied. The sets may represent products that need to be manufactured together, while the coverage requirements represent product demands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c9623",
   "metadata": {},
   "source": [
    "We consider a version of the problem where sets have non-uniform manufacturing costs, and where the demands are\n",
    "stochastic and unknown at production time. Unmet demands can then be satisfied by buying additional items, but at a\n",
    "higher cost. The requirements are generated according to a Poisson distribution, and we assume the existence of a linear\n",
    "relationships between an observable variable $o âˆˆ \\mathbb{R}$ and the Poisson rates $\\lambda$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cf675",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        \\min & \\sum_{j \\in J} c_j z_j +  \\sum_{i \\in I} p_{i} s_{i} \\label{stocmodel:costfun} \\\\\n",
    "             & \\sum_{j \\in J} a_{i, j} z_j \\geq d_{i} (1 - w_{i}) \\label{stocmodel:demands} \\quad & \\forall i \\in I \\\\ \n",
    "             & w_{i} = 1 \\implies s_{i}  \\geq d_{i} - \\sum_{j \\in J} a_{i, j} z_j \\quad & \\forall i \\in I \\label{stocmodel:indicatorconstr} \\\\\n",
    "             & z_j \\geq 0 \\\\\n",
    "             & w_{i} \\in \\left[ 0, 1 \\right] \\\\\n",
    "             & s_{i} \\geq 0 \\\\\n",
    "             & z, w \\in \\mathbb{Z}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110173f",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bf157",
   "metadata": {},
   "source": [
    "First of all, we need to generate the problem instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5224ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# FIXME: bad code here...\n",
    "# Run this cell only once to set the execution path in the main folder\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe08be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from usecases.wsmc.generate_instances import generate_training_and_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637c1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min possible value for the Poisson rates\n",
    "MIN_LMBD = 1\n",
    "# Max possible value for the Poisson rates\n",
    "MAX_LMBD = 10\n",
    "# Number of products (elements)\n",
    "NUM_PRODS = 5\n",
    "# Number of sets (molds)\n",
    "NUM_SETS = 25\n",
    "# Density of the availability matrix\n",
    "DENSITY = 0.02\n",
    "# Number of instances to generate\n",
    "NUM_INSTANCES = 1000\n",
    "# Seed to ensure reproducible results\n",
    "SEED = 0\n",
    "DATA_PATH = os.path.join('data',\n",
    "                         'wsmc',\n",
    "                         f'{NUM_PRODS}x{NUM_SETS}',\n",
    "                         'linear',\n",
    "                         f'{NUM_INSTANCES}-instances',\n",
    "                         f'seed-{SEED}')\n",
    "\n",
    "# Set the random seed to ensure reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1468490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MinSetCover] - True density: 0.264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving instances: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:24<00:00, 40.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate training and test set in the specified directory\n",
    "generate_training_and_test_sets(data_path=DATA_PATH,\n",
    "                                num_instances=NUM_INSTANCES,\n",
    "                                num_sets=NUM_SETS,\n",
    "                                num_prods=NUM_PRODS,\n",
    "                                density=DENSITY,\n",
    "                                min_lmbd=MIN_LMBD,\n",
    "                                max_lmbd=MAX_LMBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e36677",
   "metadata": {},
   "source": [
    "We can find the saved instances in the `DATA_PATH` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59814a",
   "metadata": {},
   "source": [
    "## A2C as RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e55522",
   "metadata": {},
   "source": [
    "As a baseline approached, we have employed the A2C algorithm from the `garage` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a61873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\garage\\experiment\\deterministic.py:37: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n",
      "  'Enabeling deterministic mode in PyTorch can have a performance '\n"
     ]
    }
   ],
   "source": [
    "from garage.tf.baselines import ContinuousMLPBaseline\n",
    "from garage.sampler import LocalSampler\n",
    "from garage.tf.policies import GaussianMLPPolicy\n",
    "from garage.tf.policies import Policy\n",
    "from garage.experiment import SnapshotConfig\n",
    "\n",
    "from helpers.garage_utility import CustomTFTrainer, CustomEnv, my_wrap_experiment\n",
    "from usecases.wsmc.generate_instances import MinSetCoverEnv\n",
    "from rl.algos import VPG\n",
    "from helpers.garage_utility import CustomTFTrainer, CustomEnv, my_wrap_experiment\n",
    "from helpers.utility import renamed_load\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35278b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ctxt: SnapshotConfig = None,\n",
    "          num_epochs: int = 100,\n",
    "          batch_size: int = 100,\n",
    "          env: MinSetCoverEnv = None):\n",
    "    \"\"\"\n",
    "    :param ctxt: garage.experiment.SnapshotConfig: The snapshot configuration used by Trainer to create the\n",
    "                                                   snapshotter.\n",
    "    :param num_epochs: int; number of training epochs.\n",
    "    :param batch_size: int; batch size.\n",
    "    :param env: usecases.setcover.generate_instances.MinSetCoverEnv; the Minimum Set Cover environment instance.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the env is not None\n",
    "    assert env is not None\n",
    "\n",
    "    # A trainer provides a default TensorFlow session using python context\n",
    "    with CustomTFTrainer(snapshot_config=ctxt) as trainer:\n",
    "\n",
    "        # Garage wrapping of a gym environment\n",
    "        env = CustomEnv(env, max_episode_length=1)\n",
    "\n",
    "        # A policy represented by a Gaussian distribution which is parameterized by a multilayer perceptron (MLP)\n",
    "        policy = GaussianMLPPolicy(env.spec)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        # A value function using a MLP network.\n",
    "        baseline = ContinuousMLPBaseline(env_spec=env.spec)\n",
    "\n",
    "        # It's called the \"Local\" sampler because it runs everything in the same process and thread as where\n",
    "        # it was called from.\n",
    "        sampler = LocalSampler(agents=policy,\n",
    "                               envs=env,\n",
    "                               max_episode_length=1,\n",
    "                               is_tf_worker=True)\n",
    "\n",
    "        # Vanilla Policy Gradient\n",
    "        algo = VPG(env_spec=env.spec,\n",
    "                   baseline=baseline,\n",
    "                   policy=policy,\n",
    "                   sampler=sampler,\n",
    "                   discount=0,\n",
    "                   optimizer_args=dict(learning_rate=0.001, ),\n",
    "                   center_adv=True)\n",
    "\n",
    "        trainer.setup(algo, env)\n",
    "        trainer.train(n_epochs=num_epochs, batch_size=batch_size, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0de0f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym environment - Loading instances: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:14<00:00, 69.50it/s]\n",
      "c:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:13:19 | [train] Logging to models\\5x25\\1000-instances\\seed-0\n",
      "WARNING:tensorflow:From c:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:346: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\garage\\tf\\models\\model.py:347: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2023-01-12 12:13:22 | [train] Obtaining samples...\n",
      "2023-01-12 12:13:24 | [train] epoch #0 | Optimizing policy...\n",
      "2023-01-12 12:13:24 | [train] epoch #0 | Computing loss before\n",
      "2023-01-12 12:13:24 | [train] epoch #0 | Computing KL before\n",
      "2023-01-12 12:13:24 | [train] epoch #0 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:25 | [train] epoch #0 | Computing KL after\n",
      "2023-01-12 12:13:25 | [train] epoch #0 | Computing loss after\n",
      "2023-01-12 12:13:25 | [train] epoch #0 | Fitting baseline...\n",
      "2023-01-12 12:13:25 | [train] epoch #0 | Saving snapshot...\n",
      "0.6628811359405518\n",
      "2023-01-12 12:13:26 | [train] epoch #0 | Saved\n",
      "2023-01-12 12:13:26 | [train] epoch #0 | Time 4.43 s\n",
      "2023-01-12 12:13:26 | [train] epoch #0 | EpochTime 4.43 s\n",
      "---------------------------------------  -----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       -1.02583e-05\n",
      "ContinuousMLPBaseline/LossAfter                1.02067e+08\n",
      "ContinuousMLPBaseline/LossBefore               5.15329e+08\n",
      "ContinuousMLPBaseline/dLoss                    4.13262e+08\n",
      "Evaluation/AverageDiscountedReturn        -17050.7\n",
      "Evaluation/AverageReturn                  -17050.7\n",
      "Evaluation/BatchAvgFeasibilityRatio            1\n",
      "Evaluation/BatchAvgRegret                      0\n",
      "Evaluation/BatchAvgTrueCost                17050.7\n",
      "Evaluation/Iteration                           0\n",
      "Evaluation/MaxReturn                        -327\n",
      "Evaluation/MinReturn                      -66324\n",
      "Evaluation/NumEpisodes                       100\n",
      "Evaluation/StdReturn                       14986.6\n",
      "Evaluation/TerminationRate                     0\n",
      "Extras/EpisodeRewardMean                  -17050.7\n",
      "Extras/TrueEpochTime                           3.75306\n",
      "GaussianMLPPolicy/Entropy                      7.09569\n",
      "GaussianMLPPolicy/KL                           0.0153989\n",
      "GaussianMLPPolicy/KLBefore                     0\n",
      "GaussianMLPPolicy/LossAfter              -124134\n",
      "GaussianMLPPolicy/LossBefore             -122664\n",
      "GaussianMLPPolicy/Perplexity                1206.76\n",
      "GaussianMLPPolicy/dLoss                     1469.41\n",
      "TotalEnvSteps                                100\n",
      "---------------------------------------  -----------------\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Optimizing policy...\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Computing loss before\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Computing KL before\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Computing KL after\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Computing loss after\n",
      "2023-01-12 12:13:28 | [train] epoch #1 | Fitting baseline...\n",
      "2023-01-12 12:13:29 | [train] epoch #1 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:29 | [train] epoch #1 | Saved\n",
      "2023-01-12 12:13:29 | [train] epoch #1 | Time 7.14 s\n",
      "2023-01-12 12:13:29 | [train] epoch #1 | EpochTime 2.68 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.537252\n",
      "ContinuousMLPBaseline/LossAfter               9.64923e+07\n",
      "ContinuousMLPBaseline/LossBefore              1.27408e+08\n",
      "ContinuousMLPBaseline/dLoss                   3.09154e+07\n",
      "Evaluation/AverageDiscountedReturn       -20238.4\n",
      "Evaluation/AverageReturn                 -20238.4\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               20238.4\n",
      "Evaluation/Iteration                          1\n",
      "Evaluation/MaxReturn                       -314\n",
      "Evaluation/MinReturn                     -65072\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      16485.2\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -20238.4\n",
      "Extras/TrueEpochTime                          2.65379\n",
      "GaussianMLPPolicy/Entropy                     7.0947\n",
      "GaussianMLPPolicy/KL                          0.0129148\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter              -22781\n",
      "GaussianMLPPolicy/LossBefore             -21737.1\n",
      "GaussianMLPPolicy/Perplexity               1205.56\n",
      "GaussianMLPPolicy/dLoss                    1043.87\n",
      "TotalEnvSteps                               200\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Optimizing policy...\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Computing loss before\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Computing KL before\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Computing KL after\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Computing loss after\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Fitting baseline...\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Saved\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | Time 9.56 s\n",
      "2023-01-12 12:13:31 | [train] epoch #2 | EpochTime 2.41 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.648217\n",
      "ContinuousMLPBaseline/LossAfter               7.54967e+07\n",
      "ContinuousMLPBaseline/LossBefore              1.05288e+08\n",
      "ContinuousMLPBaseline/dLoss                   2.97915e+07\n",
      "Evaluation/AverageDiscountedReturn       -16715.5\n",
      "Evaluation/AverageReturn                 -16715.5\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               16715.5\n",
      "Evaluation/Iteration                          2\n",
      "Evaluation/MaxReturn                       -246\n",
      "Evaluation/MinReturn                     -58228\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      14668.5\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -16715.5\n",
      "Extras/TrueEpochTime                          2.38215\n",
      "GaussianMLPPolicy/Entropy                     7.09291\n",
      "GaussianMLPPolicy/KL                          0.0113456\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter               20021.1\n",
      "GaussianMLPPolicy/LossBefore              20645\n",
      "GaussianMLPPolicy/Perplexity               1203.41\n",
      "GaussianMLPPolicy/dLoss                     623.889\n",
      "TotalEnvSteps                               300\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Optimizing policy...\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Computing loss before\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Computing KL before\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Computing KL after\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Computing loss after\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Fitting baseline...\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Saved\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | Time 11.96 s\n",
      "2023-01-12 12:13:33 | [train] epoch #3 | EpochTime 2.39 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.573564\n",
      "ContinuousMLPBaseline/LossAfter               9.94387e+07\n",
      "ContinuousMLPBaseline/LossBefore              1.01735e+08\n",
      "ContinuousMLPBaseline/dLoss                   2.29594e+06\n",
      "Evaluation/AverageDiscountedReturn       -14252.4\n",
      "Evaluation/AverageReturn                 -14252.4\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               14252.4\n",
      "Evaluation/Iteration                          3\n",
      "Evaluation/MaxReturn                       -352\n",
      "Evaluation/MinReturn                     -64540\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      14531.1\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -14252.4\n",
      "Extras/TrueEpochTime                          2.3655\n",
      "GaussianMLPPolicy/Entropy                     7.09045\n",
      "GaussianMLPPolicy/KL                          0.0113386\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter               -2387.01\n",
      "GaussianMLPPolicy/LossBefore              -1525.52\n",
      "GaussianMLPPolicy/Perplexity               1200.44\n",
      "GaussianMLPPolicy/dLoss                     861.492\n",
      "TotalEnvSteps                               400\n",
      "---------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:13:36 | [train] epoch #4 | Optimizing policy...\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Computing loss before\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Computing KL before\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Computing KL after\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Computing loss after\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Fitting baseline...\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Saved\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | Time 14.56 s\n",
      "2023-01-12 12:13:36 | [train] epoch #4 | EpochTime 2.58 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.56536\n",
      "ContinuousMLPBaseline/LossAfter               8.38802e+07\n",
      "ContinuousMLPBaseline/LossBefore              8.39625e+07\n",
      "ContinuousMLPBaseline/dLoss               82304\n",
      "Evaluation/AverageDiscountedReturn       -15030.2\n",
      "Evaluation/AverageReturn                 -15030.2\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               15030.2\n",
      "Evaluation/Iteration                          4\n",
      "Evaluation/MaxReturn                       -301\n",
      "Evaluation/MinReturn                     -58538\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      14292.5\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -15030.2\n",
      "Extras/TrueEpochTime                          2.53995\n",
      "GaussianMLPPolicy/Entropy                     7.08748\n",
      "GaussianMLPPolicy/KL                          0.010996\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter               -2233.4\n",
      "GaussianMLPPolicy/LossBefore              -1561.3\n",
      "GaussianMLPPolicy/Perplexity               1196.89\n",
      "GaussianMLPPolicy/dLoss                     672.103\n",
      "TotalEnvSteps                               500\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Optimizing policy...\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Computing loss before\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Computing KL before\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Computing KL after\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Computing loss after\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Fitting baseline...\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Saved\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | Time 17.28 s\n",
      "2023-01-12 12:13:39 | [train] epoch #5 | EpochTime 2.69 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.551106\n",
      "ContinuousMLPBaseline/LossAfter               8.54323e+07\n",
      "ContinuousMLPBaseline/LossBefore              9.36898e+07\n",
      "ContinuousMLPBaseline/dLoss                   8.25754e+06\n",
      "Evaluation/AverageDiscountedReturn       -11919\n",
      "Evaluation/AverageReturn                 -11919\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               11919\n",
      "Evaluation/Iteration                          5\n",
      "Evaluation/MaxReturn                       -346\n",
      "Evaluation/MinReturn                     -57218\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      13820.7\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -11919\n",
      "Extras/TrueEpochTime                          2.67164\n",
      "GaussianMLPPolicy/Entropy                     7.08421\n",
      "GaussianMLPPolicy/KL                          0.0112246\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter                8741.29\n",
      "GaussianMLPPolicy/LossBefore               9578.02\n",
      "GaussianMLPPolicy/Perplexity               1192.98\n",
      "GaussianMLPPolicy/dLoss                     836.728\n",
      "TotalEnvSteps                               600\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Optimizing policy...\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Computing loss before\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Computing KL before\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Computing KL after\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Computing loss after\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Fitting baseline...\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Saved\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | Time 19.85 s\n",
      "2023-01-12 12:13:41 | [train] epoch #6 | EpochTime 2.56 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.531746\n",
      "ContinuousMLPBaseline/LossAfter               6.37909e+07\n",
      "ContinuousMLPBaseline/LossBefore              6.74632e+07\n",
      "ContinuousMLPBaseline/dLoss                   3.67223e+06\n",
      "Evaluation/AverageDiscountedReturn       -11703.5\n",
      "Evaluation/AverageReturn                 -11703.5\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               11703.5\n",
      "Evaluation/Iteration                          6\n",
      "Evaluation/MaxReturn                       -354\n",
      "Evaluation/MinReturn                     -58844\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      11959.4\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -11703.5\n",
      "Extras/TrueEpochTime                          2.53916\n",
      "GaussianMLPPolicy/Entropy                     7.08119\n",
      "GaussianMLPPolicy/KL                          0.0106919\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter                3794.74\n",
      "GaussianMLPPolicy/LossBefore               4398.95\n",
      "GaussianMLPPolicy/Perplexity               1189.39\n",
      "GaussianMLPPolicy/dLoss                     604.207\n",
      "TotalEnvSteps                               700\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Optimizing policy...\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Computing loss before\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Computing KL before\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Computing KL after\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Computing loss after\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Fitting baseline...\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Saved\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | Time 22.41 s\n",
      "2023-01-12 12:13:44 | [train] epoch #7 | EpochTime 2.54 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.510459\n",
      "ContinuousMLPBaseline/LossAfter               8.75969e+07\n",
      "ContinuousMLPBaseline/LossBefore              8.85888e+07\n",
      "ContinuousMLPBaseline/dLoss              991912\n",
      "Evaluation/AverageDiscountedReturn       -12306.7\n",
      "Evaluation/AverageReturn                 -12306.7\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               12306.7\n",
      "Evaluation/Iteration                          7\n",
      "Evaluation/MaxReturn                       -296\n",
      "Evaluation/MinReturn                     -49261\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      13367.9\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -12306.7\n",
      "Extras/TrueEpochTime                          2.5109\n",
      "GaussianMLPPolicy/Entropy                     7.07842\n",
      "GaussianMLPPolicy/KL                          0.0110134\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter               -4336.79\n",
      "GaussianMLPPolicy/LossBefore              -3456.53\n",
      "GaussianMLPPolicy/Perplexity               1186.09\n",
      "GaussianMLPPolicy/dLoss                     880.254\n",
      "TotalEnvSteps                               800\n",
      "---------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:13:46 | [train] epoch #8 | Optimizing policy...\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Computing loss before\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Computing KL before\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Computing KL after\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Computing loss after\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Fitting baseline...\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Saved\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | Time 24.57 s\n",
      "2023-01-12 12:13:46 | [train] epoch #8 | EpochTime 2.14 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.556266\n",
      "ContinuousMLPBaseline/LossAfter               8.58754e+07\n",
      "ContinuousMLPBaseline/LossBefore              9.10435e+07\n",
      "ContinuousMLPBaseline/dLoss                   5.16808e+06\n",
      "Evaluation/AverageDiscountedReturn       -13136.7\n",
      "Evaluation/AverageReturn                 -13136.7\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost               13136.7\n",
      "Evaluation/Iteration                          8\n",
      "Evaluation/MaxReturn                       -356\n",
      "Evaluation/MinReturn                     -55864\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      14312.9\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                 -13136.7\n",
      "Extras/TrueEpochTime                          2.11282\n",
      "GaussianMLPPolicy/Entropy                     7.07562\n",
      "GaussianMLPPolicy/KL                          0.0111509\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter               -7326.46\n",
      "GaussianMLPPolicy/LossBefore              -6435.45\n",
      "GaussianMLPPolicy/Perplexity               1182.78\n",
      "GaussianMLPPolicy/dLoss                     891.012\n",
      "TotalEnvSteps                               900\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Optimizing policy...\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Computing loss before\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Computing KL before\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Computing KL after\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Computing loss after\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Fitting baseline...\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Saved\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | Time 26.86 s\n",
      "2023-01-12 12:13:48 | [train] epoch #9 | EpochTime 2.27 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.462769\n",
      "ContinuousMLPBaseline/LossAfter               5.77948e+07\n",
      "ContinuousMLPBaseline/LossBefore              7.25924e+07\n",
      "ContinuousMLPBaseline/dLoss                   1.47975e+07\n",
      "Evaluation/AverageDiscountedReturn        -9853.02\n",
      "Evaluation/AverageReturn                  -9853.02\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost                9853.02\n",
      "Evaluation/Iteration                          9\n",
      "Evaluation/MaxReturn                       -256\n",
      "Evaluation/MinReturn                     -46238\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      10996.1\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                  -9853.02\n",
      "Extras/TrueEpochTime                          2.24058\n",
      "GaussianMLPPolicy/Entropy                     7.0727\n",
      "GaussianMLPPolicy/KL                          0.0112691\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter                7915.04\n",
      "GaussianMLPPolicy/LossBefore               8611.5\n",
      "GaussianMLPPolicy/Perplexity               1179.33\n",
      "GaussianMLPPolicy/dLoss                     696.453\n",
      "TotalEnvSteps                              1000\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:50 | [train] epoch #10 | Optimizing policy...\n",
      "2023-01-12 12:13:50 | [train] epoch #10 | Computing loss before\n",
      "2023-01-12 12:13:50 | [train] epoch #10 | Computing KL before\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Computing KL after\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Computing loss after\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Fitting baseline...\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Saved\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | Time 29.17 s\n",
      "2023-01-12 12:13:51 | [train] epoch #10 | EpochTime 2.30 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.338829\n",
      "ContinuousMLPBaseline/LossAfter               6.39966e+07\n",
      "ContinuousMLPBaseline/LossBefore              6.78388e+07\n",
      "ContinuousMLPBaseline/dLoss                   3.84222e+06\n",
      "Evaluation/AverageDiscountedReturn        -9489.23\n",
      "Evaluation/AverageReturn                  -9489.23\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost                9489.23\n",
      "Evaluation/Iteration                         10\n",
      "Evaluation/MaxReturn                       -287\n",
      "Evaluation/MinReturn                     -47890\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      10215.8\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                  -9489.23\n",
      "Extras/TrueEpochTime                          2.2749\n",
      "GaussianMLPPolicy/Entropy                     7.06982\n",
      "GaussianMLPPolicy/KL                          0.0116347\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter                4553.49\n",
      "GaussianMLPPolicy/LossBefore               5380.29\n",
      "GaussianMLPPolicy/Perplexity               1175.94\n",
      "GaussianMLPPolicy/dLoss                     826.808\n",
      "TotalEnvSteps                              1100\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Optimizing policy...\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Computing loss before\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Computing KL before\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Computing KL after\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Computing loss after\n",
      "2023-01-12 12:13:54 | [train] epoch #11 | Fitting baseline...\n",
      "2023-01-12 12:13:55 | [train] epoch #11 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:55 | [train] epoch #11 | Saved\n",
      "2023-01-12 12:13:55 | [train] epoch #11 | Time 33.10 s\n",
      "2023-01-12 12:13:55 | [train] epoch #11 | EpochTime 3.92 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.561756\n",
      "ContinuousMLPBaseline/LossAfter               4.18403e+07\n",
      "ContinuousMLPBaseline/LossBefore              4.44206e+07\n",
      "ContinuousMLPBaseline/dLoss                   2.58038e+06\n",
      "Evaluation/AverageDiscountedReturn        -8143.55\n",
      "Evaluation/AverageReturn                  -8143.55\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost                8143.55\n",
      "Evaluation/Iteration                         11\n",
      "Evaluation/MaxReturn                       -338\n",
      "Evaluation/MinReturn                     -39289\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                       9144.36\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                  -8143.55\n",
      "Extras/TrueEpochTime                          3.88988\n",
      "GaussianMLPPolicy/Entropy                     7.06717\n",
      "GaussianMLPPolicy/KL                          0.0104321\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter                2478.96\n",
      "GaussianMLPPolicy/LossBefore               2889.75\n",
      "GaussianMLPPolicy/Perplexity               1172.82\n",
      "GaussianMLPPolicy/dLoss                     410.789\n",
      "TotalEnvSteps                              1200\n",
      "---------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:13:57 | [train] epoch #12 | Optimizing policy...\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Computing loss before\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Computing KL before\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Computing KL after\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Computing loss after\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Fitting baseline...\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Saving snapshot...\n",
      "0.0\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Saved\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | Time 35.20 s\n",
      "2023-01-12 12:13:57 | [train] epoch #12 | EpochTime 2.08 s\n",
      "---------------------------------------  ----------------\n",
      "ContinuousMLPBaseline/ExplainedVariance       0.524346\n",
      "ContinuousMLPBaseline/LossAfter               5.86085e+07\n",
      "ContinuousMLPBaseline/LossBefore              6.996e+07\n",
      "ContinuousMLPBaseline/dLoss                   1.13515e+07\n",
      "Evaluation/AverageDiscountedReturn        -9568.92\n",
      "Evaluation/AverageReturn                  -9568.92\n",
      "Evaluation/BatchAvgFeasibilityRatio           1\n",
      "Evaluation/BatchAvgRegret                     0\n",
      "Evaluation/BatchAvgTrueCost                9568.92\n",
      "Evaluation/Iteration                         12\n",
      "Evaluation/MaxReturn                       -366\n",
      "Evaluation/MinReturn                     -54071\n",
      "Evaluation/NumEpisodes                      100\n",
      "Evaluation/StdReturn                      11562.1\n",
      "Evaluation/TerminationRate                    0\n",
      "Extras/EpisodeRewardMean                  -9568.92\n",
      "Extras/TrueEpochTime                          2.05337\n",
      "GaussianMLPPolicy/Entropy                     7.0644\n",
      "GaussianMLPPolicy/KL                          0.00940526\n",
      "GaussianMLPPolicy/KLBefore                    0\n",
      "GaussianMLPPolicy/LossAfter              -21248.6\n",
      "GaussianMLPPolicy/LossBefore             -20635.7\n",
      "GaussianMLPPolicy/Perplexity               1169.58\n",
      "GaussianMLPPolicy/dLoss                     612.891\n",
      "TotalEnvSteps                              1300\n",
      "---------------------------------------  ----------------\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Optimizing policy...\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Computing loss before\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Computing KL before\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Optimizing\n",
      "Optimizing minibatches\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Computing KL after\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Computing loss after\n",
      "2023-01-12 12:13:59 | [train] epoch #13 | Fitting baseline...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15244\\235626502.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                          \u001b[1;31m# FIXME: archive_launch_repo=True is not supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                          archive_launch_repo=False)\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\rl-for-dfl\\helpers\\garage_utility.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mctxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15244\\2139316988.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(ctxt, num_epochs, batch_size, env)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\rl-for-dfl\\helpers\\garage_utility.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, batch_size, plot, store_episodes, pause_for_plot)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_worker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0maverage_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_algo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown_worker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\rl-for-dfl\\rl\\algos.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    192\u001b[0m                                            \u001b[0mtrain_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                                            \u001b[0mval_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                                            start_time=start_time)\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_itr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\rl-for-dfl\\rl\\algos.py\u001b[0m in \u001b[0;36m_train_once\u001b[1;34m(self, itr, episodes, train_env, val_env, start_time)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                                \u001b[0mval_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                                                \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                                                discount=self._discount)\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_reward_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundiscounted_returns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         tabular.record('Extras/EpisodeRewardMean',\n",
      "\u001b[1;32m~\\Documents\\GitHub\\rl-for-dfl\\helpers\\garage_utility.py\u001b[0m in \u001b[0;36mlog_performance\u001b[1;34m(itr, batch, validate_every_n_steps, train_env, val_env, policy, discount, prefix)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscount_cumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m         \u001b[0mundiscounted_returns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\garage\\np\\_functions.py\u001b[0m in \u001b[0;36mdiscount_cumsum\u001b[1;34m(x, discount)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \"\"\"\n\u001b[0;32m    127\u001b[0m     return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1],\n\u001b[1;32m--> 128\u001b[1;33m                                 axis=-1)[::-1]\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\documents\\github\\rl-for-dfl\\venv\\lib\\site-packages\\scipy\\signal\\signaltools.py\u001b[0m in \u001b[0;36mlfilter\u001b[1;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[0;32m   2053\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mzi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2055\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msigtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2056\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msigtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the hyper parameters\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5000\n",
    "\n",
    "SAVEPATH = os.path.join('models',\n",
    "                        f'{NUM_PRODS}x{NUM_SETS}',\n",
    "                        f'{NUM_INSTANCES}-instances',\n",
    "                        f'seed-{SEED}')\n",
    "\n",
    "# Create the environment\n",
    "env = MinSetCoverEnv(num_prods=NUM_PRODS,\n",
    "                     num_sets=NUM_SETS,\n",
    "                     instances_filepath=DATA_PATH,\n",
    "                     seed=SEED)\n",
    "\n",
    "# Create the saving directory if it does not exist\n",
    "if not os.path.exists(SAVEPATH):\n",
    "    os.makedirs(SAVEPATH)\n",
    "\n",
    "# Save the configuration params\n",
    "config_params = {'batch_size': BATCH_SIZE, 'epochs': EPOCHS}\n",
    "with open(os.path.join(SAVEPATH, 'config.yaml'), 'w') as file:\n",
    "    yaml.dump(config_params, file)\n",
    "\n",
    "# Train and test the RL algo\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "run = my_wrap_experiment(train,\n",
    "                         logging_dir=SAVEPATH,\n",
    "                         snapshot_mode='gap_overwrite',\n",
    "                         snapshot_gap=EPOCHS // 10,\n",
    "                         # FIXME: archive_launch_repo=True is not supported\n",
    "                         archive_launch_repo=False)\n",
    "run(num_epochs=EPOCHS, batch_size=BATCH_SIZE, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
